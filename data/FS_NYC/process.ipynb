{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('dataset_NYC.txt', encoding=\"unicode_escape\", names=['use_ID', 'loc_ID', 'loc_cat_ID', 'loc_cat_name', 'latitude', 'longitude', 'time_offset', 'time'], sep='\\t')\n",
    "data_exact = data[['use_ID', 'loc_ID', 'loc_cat_name', 'latitude', 'longitude', 'time_offset', 'time']]\n",
    "time = pd.to_datetime(data_exact['time'])\n",
    "time_offset = data_exact['time_offset'].values\n",
    "def local_time(x, y):\n",
    "    return x + pd.Timedelta(minutes = y)\n",
    "time_local = list(map(local_time, time, time_offset))\n",
    "data_exact['time'] = pd.DataFrame(time_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤数据\n",
    "def filter(data):\n",
    "    dm = data[['use_ID', 'loc_ID']]\n",
    "    user_cnt = dm.groupby(by=['use_ID'], as_index=False).count()\n",
    "    user_list = list(user_cnt[user_cnt['loc_ID'] >= 10]['use_ID'])#过滤掉签到次数少于10次的用户\n",
    "    ds = data[data['use_ID'].isin(user_list)]\n",
    "    loc_cnt = ds.groupby(by=['loc_ID'], as_index=False).count()\n",
    "    loc_list = list(loc_cnt[loc_cnt['use_ID'] >= 10]['loc_ID'])#过滤掉签到人数少于10人次的位置\n",
    "    data_filtered = ds[ds['loc_ID'].isin(loc_list)]\n",
    "    return data_filtered\n",
    "data_filtered = filter(data_exact)\n",
    "user_cnt = data_filtered.groupby(by=['use_ID'], as_index=False).count()\n",
    "control = True\n",
    "while control:\n",
    "    if (any(user_cnt['loc_ID'] < 10)):\n",
    "        data_filtered = filter(data_filtered)\n",
    "        user_cnt = data_filtered.groupby(by=['use_ID'], as_index=False).count()\n",
    "    else:\n",
    "        control = False\n",
    "data_filtered.sort_values(by='time', inplace=True)#按时间排序\n",
    "data_filtered.index = range(len(data_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat = pd.read_csv('FS_NYC_CAT_NAME_N.csv', encoding=\"unicode_escape\", names=['loc_cat_name', 'Quantity', 'new_cat_name'], sep=',', header=0)\n",
    "cat_map = new_cat[['loc_cat_name', 'new_cat_name']]\n",
    "cat_map.iloc[52][0] = 'Café'\n",
    "origin_cat_name = data_filtered['loc_cat_name']\n",
    "cat_map_dict = {cat_map.iloc[i][0]:cat_map.iloc[i][1] for i in range(len(cat_map))}\n",
    "loc_cat_new_name = pd.Series(map(lambda x: cat_map_dict[x], origin_cat_name))\n",
    "data_filtered.insert(7, 'loc_cat_new_name', loc_cat_new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理一个loc_ID有多个语义类别的情况，具体：位置A对应b和c两种类别，类别b包含有100个位置，类别c包含有50个位置，则将位置A设为类别b\n",
    "loc_and_cat = data_filtered[['loc_ID', 'loc_cat_new_name']].drop_duplicates()\n",
    "cat_include_loc = loc_and_cat.groupby(by='loc_cat_new_name').count()#统计各类别下包含位置的数量\n",
    "cat_include_loc.columns = ['num']\n",
    "cat_include_loc_num = {cat_include_loc.index[i]:cat_include_loc['num'][i] for i in range(len(cat_include_loc))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#筛选一个位置对应多个类别的情况\n",
    "loc_has_cat = loc_and_cat.groupby(by='loc_ID').count()\n",
    "multicat_cat_loc = loc_has_cat[loc_has_cat['loc_cat_new_name']>=2]#index:loc_ID\n",
    "#获取有多个语义的位置及其对应的语义\n",
    "multi_loc_item = loc_and_cat[loc_and_cat['loc_ID'].isin(multicat_cat_loc.index)]\n",
    "multi_loc_item = multi_loc_item.sort_values(by='loc_ID')\n",
    "multi_loc_item.index = range(len(multi_loc_item))\n",
    "mul_loc_ID = multicat_cat_loc.index.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理一个loc_ID有多个语义类别的情况，具体：位置A对应b和c两种类别，类别b包含有100个位置，类别c包含有50个位置，则将位置A设为类别c(减小类别不平衡)\n",
    "unified_cat = []\n",
    "for i, loc_id in enumerate(mul_loc_ID):\n",
    "    name = list(multi_loc_item[multi_loc_item['loc_ID'] == loc_id]['loc_cat_new_name'])\n",
    "    if len(name) == 2:\n",
    "        unified_name = name[0] if cat_include_loc_num[name[0]] < cat_include_loc_num[name[1]] else name[1]\n",
    "    else:\n",
    "        tmp_cat = name[0]\n",
    "        for i in range(len(name)-1):\n",
    "            tmp_cat = tmp_cat if cat_include_loc_num[tmp_cat] < cat_include_loc_num[name[i+1]] else name[i+1]\n",
    "        unified_name = tmp_cat\n",
    "    unified = [unified_name] * len(name)\n",
    "    # unified_cat.append(unified)\n",
    "    unified_cat = unified_cat + unified\n",
    "multi_loc_item.insert(1, 'cat_name', pd.Series(unified_cat))\n",
    "loc_new_cat = multi_loc_item[['loc_ID', 'cat_name']].drop_duplicates()\n",
    "loc_new_cat.index = range(len(loc_new_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新过滤数据中的类别信息（对应多个语义的）\n",
    "data_loc_m_to_o = pd.DataFrame()\n",
    "data_loc_m_c = data_filtered[data_filtered['loc_ID'].isin(loc_new_cat['loc_ID'])]\n",
    "for i in range(len(loc_new_cat)):\n",
    "    tmp = data_loc_m_c[data_loc_m_c['loc_ID'].isin([loc_new_cat['loc_ID'][i]])]\n",
    "    tmp['loc_cat_new_name'] = loc_new_cat['cat_name'][i]\n",
    "    data_loc_m_to_o = pd.concat([data_loc_m_to_o, tmp])\n",
    "data_loc_o_c = data_filtered[~data_filtered['loc_ID'].isin(loc_new_cat['loc_ID'])]\n",
    "data_processed = pd.concat([data_loc_o_c, data_loc_m_to_o])\n",
    "data_processed.sort_values(by='time', inplace=True)#按时间排序\n",
    "data_processed.index = range(len(data_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = data_processed[['use_ID', 'loc_ID', 'loc_cat_new_name', 'latitude', 'longitude', 'time']]\n",
    "data_processed.to_csv('FS_NYC_Cat.csv', sep=',', index=False, header=True)#原始数据过滤后添加新的类别信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集划分：按时序，8:1:1（细节：在分割点处，保证一天的完整轨迹）\n",
    "import pandas as pd\n",
    "data = pd.read_csv('FS_NYC_Cat.csv', names=['use_ID', 'loc_ID', 'loc_cat_new_name', 'latitude', 'longitude', 'time'], sep=',', header=0)\n",
    "data['time'] = pd.to_datetime(data['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.head(int(0.8*147938)-37)#-37：保证邻近分割时间点完整一天的签到记录归纳到训练集或验证集\n",
    "tmp = data.head(int(0.9*147938-77))\n",
    "valid = tmp.tail(len(tmp)-len(train))\n",
    "test = data.tail(len(data)-len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将在训练集中没有而在验证集中出现的用户ID或者位置ID的相关签到记录排除\n",
    "train_user = set(train['use_ID'])\n",
    "val_user = set(valid['use_ID'])\n",
    "test_user = set(test['use_ID'])\n",
    "train_loc = set(train['loc_ID'])\n",
    "val_loc = set(valid['loc_ID'])\n",
    "test_loc = set(test['loc_ID'])\n",
    "val_g = valid.groupby(by='use_ID')\n",
    "val_train_user = val_user - train_user\n",
    "for use_id, valg in val_g:\n",
    "    if use_id in val_train_user:\n",
    "        valid.drop(valg.index, inplace=True)\n",
    "val_g = valid.groupby(by='loc_ID')\n",
    "val_train_loc = val_loc - train_loc\n",
    "for loc_id, valg in val_g:\n",
    "    if loc_id in val_train_loc:\n",
    "        valid.drop(valg.index, inplace=True)\n",
    "        \n",
    "#将在训练集中没有而在测试集中出现的用户ID或者位置ID的相关签到记录排除\n",
    "test_g = test.groupby(by='use_ID')\n",
    "test_train_user = test_user - train_user\n",
    "for use_id, valg in test_g:\n",
    "    if use_id in test_train_user:\n",
    "        test.drop(valg.index, inplace=True)\n",
    "test_g = test.groupby(by='loc_ID')\n",
    "test_train_loc = test_loc - train_loc\n",
    "for loc_id, valg in test_g:\n",
    "    if loc_id in test_train_loc:\n",
    "        test.drop(valg.index, inplace=True)\n",
    "train.to_csv('train.csv', sep=',', index=False, header=True)\n",
    "valid.to_csv('valid.csv', sep=',', index=False, header=True)\n",
    "test.to_csv('test.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理后，验证集和测试集中出现的用户和位置均在训练集中出现过\n",
    "#可以对训练集中的用户、位置和语义类别重新编号\n",
    "import pandas as pd\n",
    "data = pd.read_csv('train.csv', names=['use_ID', 'loc_ID', 'loc_cat_new_name', 'latitude', 'longitude', 'time'], sep=',', header=0)\n",
    "user = list(set(data['use_ID']))\n",
    "user_id = {}\n",
    "for i, user in enumerate(user):\n",
    "    user_id[user] = i  #用户新编号\n",
    "user_id = pd.DataFrame(list(user_id.items()))\n",
    "user_id.columns = ['old_id', 'new_id']\n",
    "user_id.to_csv('user_id.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加类别ID和具体位置在类别内的ID\n",
    "import pandas as pd\n",
    "data = pd.read_csv('train.csv', names=['use_ID', 'loc_ID', 'loc_cat_new_name', 'latitude', 'longitude', 'time'], sep=',', header=0)\n",
    "category = data['loc_cat_new_name'].drop_duplicates()\n",
    "cat_id = {}\n",
    "for i, cat in enumerate(category):\n",
    "    cat_id[cat] = i  #语义类别编号\n",
    "#添加类别ID\n",
    "data.insert(6, 'cat_id', -1)\n",
    "for i in range(len(data)):\n",
    "    data['cat_id'][i] = cat_id[data['loc_cat_new_name'][i]]\n",
    "# cat_id = pd.DataFrame(list(cat_id.items()))\n",
    "# cat_id.columns = ['cat_name', 'cat_id']\n",
    "# cat_id.to_csv('category_id.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加位置在类别的ID\n",
    "loc_group1 = data[['loc_ID', 'loc_cat_new_name', 'cat_id']].drop_duplicates()\n",
    "loc_group2 = data[['loc_ID', 'latitude', 'longitude']].drop_duplicates()\n",
    "loc_add_catin = pd.DataFrame()\n",
    "loc_group1.insert(3, 'loc_catin_id', -1)\n",
    "loc_g1 = loc_group1.groupby(by='cat_id')\n",
    "for i, group in loc_g1:\n",
    "    group['loc_catin_id'] = range(len(group))\n",
    "    loc_add_catin = pd.concat([loc_add_catin, group])\n",
    "loc_add_catin.sort_values(by='loc_ID', inplace=True)\n",
    "loc_add_catin.index = range(len(loc_add_catin))\n",
    "#处理同一个位置ID,具有多个不同经纬度的情况\n",
    "loc_singlecoor = pd.DataFrame()\n",
    "loc_g2 = loc_group2.groupby(by='loc_ID')\n",
    "for i, group in loc_g2:\n",
    "    if len(group) == 1:\n",
    "        loc_singlecoor = pd.concat([loc_singlecoor, group])\n",
    "    else:\n",
    "        assemble = pd.DataFrame([i, group['latitude'].mean(), group['longitude'].mean()]).T\n",
    "        assemble.columns = ['loc_ID', 'latitude', 'longitude']\n",
    "        loc_singlecoor = pd.concat([loc_singlecoor, assemble])\n",
    "loc_singlecoor.index = range(len(loc_singlecoor))\n",
    "loc = pd.concat([loc_add_catin, loc_singlecoor[['latitude', 'longitude']]], axis=1)\n",
    "loc['loc_new_ID'] = loc.index\n",
    "loc.to_csv('loc.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将训练集、验证集、测试集中的用户ID和位置ID替换成新ID，同时添加类别ID和位置在类别内的ID\n",
    "import pandas as pd\n",
    "data = pd.read_csv('test.csv', names=['use_ID', 'loc_ID', 'loc_cat_new_name', 'latitude', 'longitude', 'time'], sep=',', header=0)\n",
    "loc = pd.read_csv('loc.csv', names=['loc_ID', 'loc_cat_new_name', 'cat_id', 'loc_catin_id', 'latitude', 'longitude', 'loc_new_ID'], sep=',', header=0)\n",
    "user = pd.read_csv('user_id.csv', names=['old_id', 'new_id'], sep=',', header=0)\n",
    "user_id = {}\n",
    "loc_id = {}\n",
    "for i in range(len(user)):\n",
    "    user_id[user['old_id'][i]] = user['new_id'][i]\n",
    "for i in range(len(loc)):\n",
    "    loc_id[loc['loc_ID'][i]] = loc.loc[i, ['loc_new_ID', 'cat_id', 'loc_catin_id']]\n",
    "    # loc_id[loc['loc_ID'][i]] = loc['loc_new_ID'][i]\n",
    "data['user_new_ID'] = -1\n",
    "data['loc_new_ID'] = -1\n",
    "data['cat_id'] = -1\n",
    "data['loc_catin_id'] = -1\n",
    "for i in range(len(data)):\n",
    "    data['user_new_ID'][i] = user_id[data['use_ID'][i]]  \n",
    "    data['loc_new_ID'][i] = loc_id[data['loc_ID'][i]][0]\n",
    "    data['cat_id'][i] = loc_id[data['loc_ID'][i]][1]\n",
    "    data['loc_catin_id'][i] = loc_id[data['loc_ID'][i]][2]\n",
    "train_data = data[['user_new_ID', 'loc_new_ID', 'time', 'cat_id', 'loc_catin_id']]\n",
    "train_data.to_csv('NYC_test.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将训练集、验证集、测试集签到数据按用户，连续两次签到间隔超过24小时，拆分为不同的轨迹\n",
    "import pandas as pd\n",
    "data = pd.read_csv('NYC_test.csv', names=['user_new_ID', 'loc_new_ID', 'time', 'cat_id', 'loc_catin_id'], sep=',', header=0)\n",
    "data.sort_values(['user_new_ID','time'], ascending=[True, True], inplace=True)\n",
    "data.index = range(len(data))\n",
    "#生成每条签到记录的轨迹号\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data_g = data.groupby(by='user_new_ID')\n",
    "data_traj = pd.DataFrame()\n",
    "for user, datag in data_g:\n",
    "    traj = 0\n",
    "    datag['traj_num'] = -1\n",
    "    datag.index = range(len(datag))\n",
    "    f = 0\n",
    "    for i in range(len(datag)-1):\n",
    "        deltatime = datag['time'][i+1] - datag['time'][i]\n",
    "        if deltatime.days >= 1:\n",
    "            datag['traj_num'][f:i+1] = traj\n",
    "            f = i + 1\n",
    "            traj += 1\n",
    "    datag['traj_num'][f:] = traj\n",
    "    data_traj = pd.concat((data_traj, datag))\n",
    "data_traj.index = range(len(data_traj))\n",
    "#生成每条签到记录所在的星期几名字\n",
    "week = [data_traj['time'][i].isoweekday() for i in range(len(data_traj))]\n",
    "data_traj['day_of_week'] = week\n",
    "#生成每条签到记录的时间片\n",
    "tm = data_traj['time']\n",
    "tms = list(map(lambda x: x.hour * 2 + 2 if x.minute >=30 else x.hour * 2 + 1, tm))\n",
    "data_traj['timeslot'] = pd.Series(tms)\n",
    "data_traj.to_csv('test_traj.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_traj_g = data_traj.groupby(by=['user_new_ID'])\n",
    "# traj_all = []\n",
    "# # traj_all = dict()\n",
    "# for i, u_traj in data_traj_g:\n",
    "#     u_traj_g = u_traj.groupby(by=['traj_num'])\n",
    "#     traj_src = dict()\n",
    "#     traj_pred = dict()\n",
    "#     for n, d_traj in u_traj_g:\n",
    "#         d_t = d_traj[['loc_new_ID', 'timeslot', 'day_of_week', 'cat_id', 'loc_catin_id']]\n",
    "#         if len(d_t) > 1: #排除只有一个轨迹点的轨迹\n",
    "#             traj_points = [(d_t.iloc[j][0], d_t.iloc[j][1], d_t.iloc[j][2], d_t.iloc[j][3], d_t.iloc[j][4]) for j in range(len(d_t))]\n",
    "#         # if len(traj_points) > 1:\n",
    "#             traj_src[n] = traj_points[:-1]\n",
    "#             traj_pred[n] = traj_points[-1]\n",
    "#     # traj_all[i] = {'forward': traj_src, 'pred': traj_pred}\n",
    "#     if len(traj_pred) > 0:\n",
    "#         traj_all.append({i: {'forward': traj_src, 'pred': traj_pred}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取轨迹内的转移边，构造全局转移图 \n",
    "import pandas as pd\n",
    "data = pd.read_csv('TKY_train.csv', names=['user_new_ID', 'loc_new_ID', 'time'], sep=',', header=0)\n",
    "data.sort_values(['user_new_ID','time'], ascending=[True, True], inplace=True)\n",
    "data.index = range(len(data))\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data_g = data.groupby(by='user_new_ID')\n",
    "data_traj = pd.DataFrame()\n",
    "#提取用户的各条轨迹\n",
    "for user, datag in data_g:\n",
    "    traj = 0\n",
    "    datag['traj_num'] = -1\n",
    "    datag.index = range(len(datag))\n",
    "    f = 0\n",
    "    for i in range(len(datag)-1):\n",
    "        deltatime = datag['time'][i+1] - datag['time'][i]\n",
    "        if deltatime.days >= 1:\n",
    "            datag['traj_num'][f:i+1] = traj\n",
    "            f = i + 1\n",
    "            traj += 1\n",
    "    datag['traj_num'][f:] = traj\n",
    "    data_traj = pd.concat((data_traj, datag))\n",
    "data_traj.index = range(len(data_traj))\n",
    "\n",
    "#提取轨迹内的转移边：共82965条转移连边（训练集）\n",
    "data_traj_g = data_traj.groupby(by=['user_new_ID', 'traj_num'])\n",
    "edge_trans = []\n",
    "for i, traj in data_traj_g:\n",
    "    traj.index = range(len(traj))\n",
    "    trans_e = [(traj['loc_new_ID'][j], traj['loc_new_ID'][j+1]) for j in range(len(traj) - 1)]\n",
    "    edge_trans += trans_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计转移边及其频次 有向边：src->dst：频次\n",
    "from collections import Counter\n",
    "edge_trans_cnt = Counter(edge_trans)#最大频次为\n",
    "global_tran_e = dict(edge_trans_cnt)\n",
    "global_tran_edge = pd.DataFrame(global_tran_e.keys())\n",
    "global_tran_edge.columns = ['src', 'dst']\n",
    "global_tran_edge['freq'] = global_tran_e.values()\n",
    "gtedge_g = global_tran_edge.groupby(by='dst')\n",
    "tran_edge = pd.DataFrame()\n",
    "for dst, e in gtedge_g:\n",
    "    e.index = range(len(e))\n",
    "    total_freq = e['freq'].sum()\n",
    "    e['weight'] = e['freq'] / total_freq\n",
    "    tran_edge = pd.concat((tran_edge, e))\n",
    "tran_edge.to_csv('tran_edge.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建位置的地理空间连边  haversine距离小于1000m\n",
    "import pandas as pd\n",
    "import transbigdata as tbd\n",
    "import geohash as gh\n",
    "from haversine import haversine\n",
    "data = pd.read_csv('loc.csv', names=['loc_ID', 'latitude', 'longitude', 'loc_new_ID'], sep=',', header=0)\n",
    "loc = data[['loc_new_ID', 'latitude', 'longitude']]\n",
    "geohash = tbd.geohash_encode(loc['longitude'], loc['latitude'], precision=5)#precision:5(4.89*4.89km),6(1.22*0.61km)\n",
    "geo_edge = []\n",
    "for i in range(len(loc)):\n",
    "    neigh = gh.neighbors(geohash[i])\n",
    "    for k in neigh:\n",
    "        loc_geo = geohash[geohash.values == neigh[k]]\n",
    "        if len(loc_geo) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # for j in loc_geo.index:\n",
    "            #     dist = haversine((loc['latitude'][i], loc['longitude'][i]), (loc['latitude'][j], loc['longitude'][j]), unit='m')\n",
    "            #     if dist <= 1000:#超参：距离小于1000m的两个位置建立连边\n",
    "            #         geo_edge +=[(i, j)]\n",
    "            geo_edge += [(i, j) for j in loc_geo.index if haversine((loc['latitude'][i], loc['longitude'][i]), (loc['latitude'][j], loc['longitude'][j]), unit='m')<=1000]\n",
    "geo_edge = pd.DataFrame(geo_edge)\n",
    "geo_edge.columns = ['src', 'dst']\n",
    "geo_edge.to_csv('geo_edge.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建同类别连边，目的：建模层次约束关系\n",
    "#统计32个语义类别下位置的最高数量\n",
    "import pandas as pd\n",
    "data = pd.read_csv('loc.csv', names=['loc_ID', 'loc_cat_new_name', 'cat_id', 'loc_catin_id','latitude', 'longitude', 'loc_new_ID'], sep=',', header=0)\n",
    "data.sort_values(by=['cat_id', 'loc_catin_id'], ascending=[True, True], inplace=True)\n",
    "data.index = range(len(data))\n",
    "data_g = data.groupby(by='cat_id')\n",
    "cat_edge = []\n",
    "for i, group in data_g:\n",
    "    loc_new_id = group['loc_new_ID']\n",
    "    loc_new_id.index = range(len(loc_new_id))\n",
    "    for j in range(len(loc_new_id)):\n",
    "        cat_edge +=([(loc_new_id[j], loc_new_id[k]) for k in range(len(loc_new_id))])\n",
    "cat_edge = pd.DataFrame(cat_edge)\n",
    "cat_edge.columns = ['src', 'dst']\n",
    "cat_edge.to_csv('cat_edge.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "traj_data = pd.read_csv('train_traj.csv', names=['user_new_ID','loc_new_ID','time', 'cat_id', 'loc_catin_id', 'traj_num','day_of_week','timeslot'], sep=',', header=0)\n",
    "data_traj_g = traj_data.groupby(by=['user_new_ID', 'traj_num'])\n",
    "traj_f = []\n",
    "traj_p = []\n",
    "traj_user = []\n",
    "traj_user_traj_n = []\n",
    "for i, traj in data_traj_g:\n",
    "    d_t = np.array(traj[['loc_new_ID', 'timeslot', 'day_of_week', 'cat_id', 'loc_catin_id']])\n",
    "    if len(d_t) > 1:#剔除只有一条签到记录的轨迹\n",
    "        traj_tmp = [tuple(d_t[i]) for i in range(len(d_t))]\n",
    "        traj_f.append(traj_tmp[:-1])\n",
    "        # traj_p.append(traj_tmp[-1])#只提取最后一次\n",
    "        traj_p.append(traj_tmp[1:])#从第2个签到记录开到最后\n",
    "        traj_user.append(i[0])\n",
    "        traj_user_traj_n.append(i[1])\n",
    "file = open('train_forward.pickle','wb')\n",
    "pickle.dump(traj_f,file)\n",
    "file.close()\n",
    "file = open('train_labels.pickle','wb')\n",
    "pickle.dump(traj_p,file)\n",
    "file.close()\n",
    "file = open('train_user.pickle','wb')\n",
    "pickle.dump(traj_user,file)\n",
    "file.close()\n",
    "# traj_p = pd.DataFrame(traj_p)\n",
    "# traj_p.columns = ['loc_new_ID', 'timeslot', 'day_of_week']\n",
    "# traj_user = pd.DataFrame(traj_user)\n",
    "# traj_user.columns = ['user_new_ID']\n",
    "# traj_pred = pd.concat((traj_user, traj_p), 1)\n",
    "# traj_pred.to_csv('valid_pred.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计轨迹平均长度\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_data = pd.read_csv('train_traj.csv', names=['user_new_ID','loc_new_ID','time','traj_num','day_of_week','timeslot'], sep=',', header=0)\n",
    "valid_data = pd.read_csv('valid_traj.csv', names=['user_new_ID','loc_new_ID','time','traj_num','day_of_week','timeslot'], sep=',', header=0)\n",
    "test_data = pd.read_csv('test_traj.csv', names=['user_new_ID','loc_new_ID','time','traj_num','day_of_week','timeslot'], sep=',', header=0)\n",
    "train_data_g = train_data.groupby(by=['user_new_ID', 'traj_num'])\n",
    "valid_data_g = valid_data.groupby(by=['user_new_ID', 'traj_num'])\n",
    "test_data_g = test_data.groupby(by=['user_new_ID', 'traj_num'])\n",
    "train_data_len = []\n",
    "valid_data_len = []\n",
    "test_data_len = []\n",
    "for i, traj in train_data_g:\n",
    "    d_t = np.array(traj[['loc_new_ID', 'timeslot', 'day_of_week']])\n",
    "    if len(d_t) > 1:\n",
    "        train_data_len.append(len(d_t))\n",
    "for i, traj in valid_data_g:\n",
    "    d_t = np.array(traj[['loc_new_ID', 'timeslot', 'day_of_week']])\n",
    "    if len(d_t) > 1:\n",
    "        valid_data_len.append(len(d_t))\n",
    "for i, traj in test_data_g:\n",
    "    d_t = np.array(traj[['loc_new_ID', 'timeslot', 'day_of_week']])\n",
    "    if len(d_t) > 1:\n",
    "        test_data_len.append(len(d_t))\n",
    "#平均轨迹长度为5.2\n",
    "mean_len = (sum(train_data_len)+sum(valid_data_len)+sum(test_data_len))/(len(train_data_len)+len(valid_data_len)+len(test_data_len))\n",
    "print(mean_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "loc = pd.read_csv('loc.csv', names=['loc_ID', 'loc_cat_new_name', 'cat_id', 'loc_catin_id', 'latitude', 'longitude', 'loc_new_ID'], sep=',', header=0)\n",
    "loc_cat = loc[['loc_new_ID', 'cat_id']]\n",
    "loc_cat.sort_values(by='loc_new_ID', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_map_cat = {}\n",
    "for i in range(len(loc_cat)):\n",
    "    loc_map_cat[i] = loc_cat['cat_id'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = [1, 2, 3]\n",
    "b = [2, 3, 4]\n",
    "a = torch.tensor(a)\n",
    "b = torch.tensor(b)\n",
    "torch.matmul(a,b)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1033eb28085bb0a582f96e806060dbf3f27ac1b7abb68d3449d2cbe540e0310"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('Ada-gat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1033eb28085bb0a582f96e806060dbf3f27ac1b7abb68d3449d2cbe540e0310"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
